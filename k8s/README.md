# RunPath Kubernetes Configuration

Esta carpeta contiene la definici√≥n declarativa de la infraestructura de la aplicaci√≥n RunPath con **Network Segmentation Pattern** implementado mediante namespaces y NetworkPolicies.

## üîí Arquitectura de Seguridad

RunPath implementa una arquitectura de **segmentaci√≥n por tiers** que replica y mejora el patr√≥n que anteriormente us√°bamos con Docker networks. Cada tier est√° aislado en su propio namespace y solo puede comunicarse con los tiers adyacentes mediante NetworkPolicies estrictas.

**Flujo de tr√°fico:**
```
Internet ‚Üí [Security/Ingress] ‚Üí [Presentation/Edge] ‚Üí [Orchestration] ‚Üí [Backend] ‚Üí [Data]
```

üìñ **Documentaci√≥n completa**:
- [NETWORK-SEGMENTATION-IMPLEMENTATION.md](./NETWORK-SEGMENTATION-IMPLEMENTATION.md) - ‚≠ê **NUEVO:** Documentaci√≥n completa del patr√≥n implementado
- [NETWORK-SEGMENTATION.md](./NETWORK-SEGMENTATION.md) - Arquitectura y deployment (original)
- [TESTING-NETWORK-SEGMENTATION.md](./TESTING-NETWORK-SEGMENTATION.md) - Pruebas de validaci√≥n
- [CLEANUP-GUIDE.md](./CLEANUP-GUIDE.md) - Gu√≠a para eliminar recursos duplicados

## Estructura

La configuraci√≥n est√° organizada por tipo de recurso para facilitar la navegaci√≥n y el mantenimiento:

### Namespaces por Tier
*   **`namespaces/`**: Definici√≥n de los 6 namespaces que segmentan la aplicaci√≥n por tiers de seguridad
    - `ingress-nginx` (Tier 0) - Ingress Controller NGINX ‚ö†Ô∏è **namespace real del tier 0**
    - `security` (Tier 0) - WAF y Security Gateway (actualmente vac√≠o - legacy)
    - `presentation` (Tier 1) - Frontend Web
    - `edge` (Tier 2) - Mobile Gateway (actualmente sin pods)
    - `orchestration` (Tier 3) - API Gateway
    - `backend` (Tier 5) - Microservices y RabbitMQ
    - `data` (Tier 7) - PostgreSQL (‚ö†Ô∏è temporalmente en `default` por migraci√≥n PVC)

### Recursos de Aplicaci√≥n
*   **`deployments/`**: Definiciones de los microservicios y aplicaciones
    - `frontend-deployment.yaml` (‚Üí namespace `presentation`)
    - `api-gateway-deployment.yaml` (‚Üí namespace `orchestration`)
    - `auth-deployment.yaml` (‚Üí namespace `backend`)
    - `routes-deployment.yaml` (‚Üí namespace `backend`)
    - `distance-deployment.yaml` (‚Üí namespace `backend`)
    - `notification-deployment.yaml` (‚Üí namespace `backend`)
    - `postgres-deployment.yaml` (‚Üí namespace `data`)

*   **`statefulsets/`**: Aplicaciones con estado
    - `rabbitmq.yaml` (‚Üí namespace `backend`)

*   **`services/`**: Servicios ClusterIP para comunicaci√≥n interna (cada uno en su namespace correcto)

*   **`ingresses/`**: Reglas de entrada p√∫blica
    - `frontend-ingress.yaml` (‚Üí `presentation` namespace)
    - `mobile-ingress.yaml` (‚Üí `orchestration` namespace)

*   **`configmaps/`**: Configuraci√≥n de aplicaciones (migrados a sus namespaces)
    - Usan FQDNs para cross-namespace: `service.namespace.svc.cluster.local`

### Network Policies (‚ö†Ô∏è Cr√≠tico para Seguridad)
*   **`network-policies/`**: Pol√≠ticas de segmentaci√≥n de red
    - `tier-segmentation.yaml`: NetworkPolicies que implementan el patr√≥n de segmentaci√≥n
        - Default Deny All en cada namespace
        - Allow solo entre tiers adyacentes
        - Excepciones para DNS y comunicaci√≥n interna

## üöÄ Deployment

### Orden de aplicaci√≥n IMPORTANTE:

```bash
# 1. Crear namespaces primero
kubectl apply -f namespaces/namespaces.yaml

# 2. ConfigMaps (antes de deployments que los referencian)
kubectl apply -f configmaps/

# 3. Data tier primero (stateful)
kubectl apply -f deployments/postgres-deployment.yaml
kubectl apply -f services/postgres.yaml

# 4. Backend tier
kubectl apply -f statefulsets/rabbitmq.yaml
kubectl apply -f services/rabbitmq.yaml
kubectl apply -f deployments/auth-deployment.yaml
kubectl apply -f deployments/routes-deployment.yaml
kubectl apply -f deployments/distance-deployment.yaml
kubectl apply -f deployments/notification-deployment.yaml
kubectl apply -f services/auth-service.yaml
kubectl apply -f services/routes-service.yaml
kubectl apply -f services/distance-service.yaml
kubectl apply -f services/notification-deployment-service.yaml

# 5. Orchestration tier
kubectl apply -f deployments/api-gateway-deployment.yaml
kubectl apply -f services/api-gateway-service.yaml

# 6. Presentation tier
kubectl apply -f deployments/frontend-deployment.yaml
kubectl apply -f services/frontend-deployment-service.yaml

# 7. Ingresses
kubectl apply -f ingresses/frontend-ingress.yaml
kubectl apply -f ingresses/mobile-ingress.yaml

# 8. Network Policies (AL FINAL para no bloquear durante deployment)
kubectl apply -f network-policies/tier-segmentation.yaml
```

### Aplicaci√≥n r√°pida (una vez que el orden est√© claro):
```bash
# Aplicar toda la configuraci√≥n
kubectl apply -f namespaces/
kubectl apply -f configmaps/
kubectl apply -f deployments/
kubectl apply -f statefulsets/
kubectl apply -f services/
kubectl apply -f ingresses/
kubectl apply -f network-policies/
```

## üîç Verificaci√≥n

```bash
# Ver estado de todos los namespaces
kubectl get namespaces --show-labels

# Ver pods por tier
kubectl get pods -n ingress-nginx  # Tier 0
kubectl get pods -n presentation
kubectl get pods -n orchestration
kubectl get pods -n backend
kubectl get pods -n default  # PostgreSQL temporal

# Ver Network Policies aplicadas (deber√≠an ser 18)
kubectl get networkpolicies -A

# Probar conectividad permitida (frontend ‚Üí api-gateway)
kubectl exec -it -n presentation <frontend-pod> -- wget -O- http://api-gateway-service.orchestration.svc.cluster.local:80/health

# Verificar que conectividad prohibida falla (presentation ‚Üí default/postgres)
kubectl exec -it -n presentation <frontend-pod> -- wget -O- --timeout=5 http://postgres.default.svc.cluster.local:5432
# Debe fallar por Network Policy (timeout esperado)
```

## Convenciones

*   **Namespaces**: Cada componente est√° en el namespace de su tier de seguridad
*   **Labels**: Todos los recursos tienen labels `tier` y `tier-level` para identificaci√≥n
*   **FQDNs**: Las referencias cross-namespace usan FQDNs completos: `service.namespace.svc.cluster.local`
*   **Services**: Todos ClusterIP (internos), excepto LoadBalancer/NodePort si es necesario
*   **Limpieza**: Los archivos YAML no contienen campos de estado (`status`), UIDs, ni `resourceVersion`
*   **Seguridad**: Network Policies en modo "default deny" con allows expl√≠citos

## üìö Documentaci√≥n Adicional

- **[NETWORK-SEGMENTATION.md](./NETWORK-SEGMENTATION.md)**: Documentaci√≥n completa del patr√≥n de segmentaci√≥n
- Detalles de cada tier y sus componentes
- Mapeo desde Docker networks a Kubernetes namespaces
- Troubleshooting y verificaci√≥n
- Security benefits y pr√≥ximos pasos

## ‚ö†Ô∏è Notas Importantes

1. **Network Policies**: Aplicar AL FINAL del deployment para evitar bloquear pods durante la creaci√≥n
2. **ConfigMaps**: Deben existir ANTES de crear deployments que los referencian
3. **Secrets**: No est√°n en este repo (se crean manualmente o con herramientas de CI/CD)
4. **DNS**: Los pods pueden tardar unos segundos en resolver FQDNs despu√©s de crearse los services
5. **RabbitMQ**: Es un StatefulSet que requiere almacenamiento persistente configurado en el cluster

## üö® Problemas Conocidos (Estado Actual)

1. **Distance Service**: Pod en estado `Pending` por recursos insuficientes en el cluster (requiere scale-up de nodos o liberar recursos)
2. **Namespace `security`**: Est√° vac√≠o - el Tier 0 usa `ingress-nginx` namespace (considerar eliminar `security` o consolidar)
3. **PostgreSQL en `default`**: Por migraci√≥n de PVC pendiente - funciona con NetworkPolicy especial `backend-allow-to-default-postgres`
4. **Calico Typha**: 1 r√©plica en Pending (no cr√≠tico - 1 r√©plica funcional es suficiente para 2 nodos)
