# RunPath Network Segmentation Pattern - ImplementaciÃ³n Completa

**Ãšltima actualizaciÃ³n**: 8 de diciembre, 2025

---

## ğŸ¯ Objetivo

Este documento describe la **implementaciÃ³n completa del Network Segmentation Pattern** en RunPath, que replica y mejora la arquitectura de segmentaciÃ³n por capas que se tenÃ­a en Docker usando redes privadas, pero ahora con Kubernetes nativo usando **Namespaces + NetworkPolicies + Ingress**.

---

## ğŸ“š Conceptos Clave

### Del patrÃ³n Docker al patrÃ³n Kubernetes

#### **Antes (Docker):**
```
Internet â†’ public_net (172.26.0.0/16)
         â†’ frontend_net (172.27.0.0/16)
         â†’ orchestration_net (172.29.0.0/16)
         â†’ backend_net (172.28.0.0/16)
         â†’ db_net (172.30.0.0/16)
```

Cada servicio conectado **solo a las redes que necesitaba**, creando aislamiento fÃ­sico de red.

#### **Ahora (Kubernetes):**
```
Internet â†’ Ingress Controller (namespace: ingress-nginx, tier: security)
         â†’ Frontend SSR (namespace: presentation, tier: presentation)
         â†’ API Gateway (namespace: orchestration, tier: orchestration)
         â†’ Microservices (namespace: backend, tier: backend)
         â†’ Databases (namespace: default*, tier: data)
```

Aislamiento lÃ³gico mediante **NetworkPolicies** que controlan trÃ¡fico entre namespaces basado en labels.

---

## ğŸ—ï¸ Arquitectura Implementada

### Namespaces por Tier

| Tier Level | Tier Name | Namespace | Componentes | FunciÃ³n |
|------------|-----------|-----------|-------------|---------|
| **Tier 0** | Security | `ingress-nginx` | NGINX Ingress Controller | TLS termination, WAF, Rate limiting |
| **Tier 1** | Presentation | `presentation` | Frontend SSR (NextJS) | Interfaz web pÃºblica |
| **Tier 2** | Edge | `edge` | Mobile Reverse Proxy | Gateway para apps mÃ³viles |
| **Tier 3** | Orchestration | `orchestration` | API Gateway | Enrutamiento y composiciÃ³n de APIs |
| **Tier 5** | Logic | `backend` | Auth, Routes, Distance, Notifications, RabbitMQ | LÃ³gica de negocio |
| **Tier 7** | Data | `default`* | PostgreSQL + PostGIS | Persistencia de datos |

> **\*** PostgreSQL estÃ¡ temporalmente en `default` por razones de migraciÃ³n de PVC. Las NetworkPolicies permiten acceso controlado desde `backend`.

### Labels de SegmentaciÃ³n

Todos los recursos tienen labels consistentes:

```yaml
metadata:
  labels:
    tier: <tier-name>           # security|presentation|orchestration|backend|data
    tier-level: <tier-number>   # 0|1|2|3|5|7
    app: <app-name>             # frontend, api-gateway, auth, etc.
```

---

## ğŸ”’ Reglas de SegmentaciÃ³n

### Principio Fundamental

> **Solo el tier adyacente puede comunicarse con el siguiente tier interno**

### Flujo de TrÃ¡fico Permitido

```
Internet (pÃºblico)
  â†“
[Tier 0: Security/Ingress]  â† TLS, WAF, Rate Limiting
  â†“
[Tier 1: Presentation]  â† Solo HTTP interno desde Ingress
  â†“
[Tier 3: Orchestration]  â† Solo desde Presentation/Edge
  â†“
[Tier 5: Backend]  â† Solo desde Orchestration
  â†“
[Tier 7: Data]  â† Solo desde Backend
```

### TrÃ¡fico Bloqueado (Ejemplos)

- âŒ Internet â†’ Backend directamente
- âŒ Presentation â†’ Backend (saltar Orchestration)
- âŒ Presentation â†’ Data (saltar mÃºltiples tiers)
- âŒ Backend â†’ Presentation (movimiento lateral)

---

## ğŸ›¡ï¸ NetworkPolicies Implementadas

### 1. Default Deny (Base Security)

Cada namespace tiene una polÃ­tica de **deny-all** por defecto:

```yaml
# Ejemplo: presentation-default-deny
spec:
  podSelector: {}  # Aplica a todos los pods
  policyTypes:
  - Ingress
  - Egress
```

**Efecto**: Todo el trÃ¡fico entrante y saliente estÃ¡ bloqueado hasta que se permita explÃ­citamente.

### 2. Allow Policies (ComunicaciÃ³n Controlada)

#### Tier 1 (Presentation) â†’ Tier 3 (Orchestration)

```yaml
# presentation-allow-to-orchestration
spec:
  podSelector:
    matchLabels:
      tier: presentation
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          tier: orchestration
    ports:
    - protocol: TCP
      port: 80
```

#### Tier 3 (Orchestration) â†’ Tier 5 (Backend)

```yaml
# orchestration-allow-to-backend
spec:
  podSelector:
    matchLabels:
      tier: orchestration
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          tier: backend
    ports:
    - protocol: TCP
      port: 80
```

#### Tier 5 (Backend) â†’ Tier 7 (Data - Postgres en default)

```yaml
# backend-allow-to-default-postgres (TEMPORAL)
spec:
  podSelector:
    matchLabels:
      tier: backend
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: default
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432
```

### 3. DNS Resolution

Todos los tiers permiten Egress a `kube-system` en puerto UDP/53 para resoluciÃ³n DNS:

```yaml
egress:
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: kube-system
  ports:
  - protocol: UDP
    port: 53
```

---

## ğŸ” VerificaciÃ³n de SegmentaciÃ³n

### Pruebas de Conectividad Permitida

#### 1. Presentation â†’ Orchestration âœ…

```powershell
kubectl exec -n presentation <frontend-pod> -- \
  wget -O- --timeout=5 http://api-gateway-service.orchestration.svc.cluster.local
```

**Esperado**: ConexiÃ³n exitosa (puede ser 404 si no existe el endpoint, pero la conexiÃ³n TCP debe establecerse).

#### 2. Orchestration â†’ Backend âœ…

```powershell
kubectl exec -n orchestration <api-gateway-pod> -- \
  wget -O- --timeout=5 http://auth-service.backend.svc.cluster.local/health
```

**Esperado**: ConexiÃ³n exitosa.

#### 3. Backend â†’ Postgres (default) âœ…

```powershell
kubectl exec -n backend <routes-pod> -- \
  nc -w 3 postgres.default.svc.cluster.local 5432
```

**Esperado**: ConexiÃ³n exitosa.

### Pruebas de SegmentaciÃ³n (TrÃ¡fico Bloqueado)

#### 1. Presentation â†’ Data âŒ

```powershell
kubectl exec -n presentation <frontend-pod> -- \
  timeout 5 wget -O- http://postgres.default.svc.cluster.local:5432
```

**Esperado**: Timeout (NetworkPolicy bloqueando).

#### 2. Presentation â†’ Backend âŒ

```powershell
kubectl exec -n presentation <frontend-pod> -- \
  timeout 5 wget -O- http://auth-service.backend.svc.cluster.local
```

**Esperado**: Timeout (debe pasar por Orchestration).

---

## ğŸ“Š Estado Actual del Cluster

### Pods por Namespace

```
presentation/
â”œâ”€â”€ frontend-deployment (3 replicas) âœ… Running

orchestration/
â”œâ”€â”€ api-gateway-deployment (1 replica) âœ… Running

backend/
â”œâ”€â”€ auth-deployment (3 replicas) âœ… Running
â”œâ”€â”€ routes-deployment (1 replica) âœ… Running
â”œâ”€â”€ distance-deployment (1 replica) âœ… Running
â”œâ”€â”€ notification-deployment (3 replicas) âœ… Running
â””â”€â”€ rabbitmq StatefulSet (3 replicas) âœ… Running

default/
â””â”€â”€ postgres-deployment (1 replica) âœ… Running (TEMPORAL)
```

**Total pods de aplicaciÃ³n**: ~18 pods Running

### NetworkPolicies Aplicadas

```powershell
kubectl get networkpolicies -A
```

**Por namespace**:
- `presentation`: 3 policies (deny-all, allow-from-security, allow-to-orchestration)
- `orchestration`: 3 policies (deny-all, allow-from-presentation-edge, allow-to-backend)
- `backend`: 4 policies (deny-all, allow-from-orchestration, allow-internal, allow-to-default-postgres)
- `default`: 1 policy (allow-postgres-from-backend)
- `edge`: 3 policies (deny-all, allow-from-security, allow-to-orchestration)

**Total**: 14 NetworkPolicies activas

### PVCs y Almacenamiento

```
backend/
â”œâ”€â”€ rabbitmq-data-rabbitmq-0 (10Gi) âœ… Bound
â”œâ”€â”€ rabbitmq-data-rabbitmq-1 (10Gi) âœ… Bound
â””â”€â”€ rabbitmq-data-rabbitmq-2 (10Gi) âœ… Bound

default/
â””â”€â”€ postgres-pvc (5Gi) âœ… Bound
```

**Total SSD usado**: ~35GB / 250GB (14%)

---

## ğŸš€ Deployment del PatrÃ³n

### Orden de AplicaciÃ³n

```powershell
# 1. Namespaces y labels
kubectl apply -f namespaces/namespaces.yaml
kubectl label namespace ingress-nginx tier=security tier-level=tier-0 --overwrite

# 2. ConfigMaps (antes de deployments)
kubectl apply -f configmaps/

# 3. Data tier (stateful primero)
kubectl apply -f deployments/postgres-deployment.yaml
kubectl apply -f services/postgres.yaml

# 4. Backend tier
kubectl apply -f statefulsets/rabbitmq.yaml
kubectl apply -f services/rabbitmq.yaml
kubectl apply -f deployments/auth-deployment.yaml
kubectl apply -f deployments/routes-deployment.yaml
kubectl apply -f deployments/distance-deployment.yaml
kubectl apply -f deployments/notification-deployment.yaml
kubectl apply -f services/auth-service.yaml
kubectl apply -f services/routes-service.yaml
kubectl apply -f services/distance-service.yaml
kubectl apply -f services/notification-deployment-service.yaml

# 5. Orchestration tier
kubectl apply -f deployments/api-gateway-deployment.yaml
kubectl apply -f services/api-gateway-service.yaml

# 6. Presentation tier
kubectl apply -f deployments/frontend-deployment.yaml
kubectl apply -f services/frontend-deployment-service.yaml

# 7. Ingresses
kubectl apply -f ingresses/frontend-ingress.yaml
kubectl apply -f ingresses/mobile-ingress.yaml

# 8. NetworkPolicies (AL FINAL)
kubectl apply -f network-policies/tier-segmentation.yaml
kubectl apply -f network-policies/allow-backend-to-default-postgres.yaml
```

> âš ï¸ **Importante**: Aplicar NetworkPolicies **AL FINAL** para no bloquear pods durante el deployment inicial.

---





## ğŸ“‹ TODOs y Mejoras Futuras

## âš ï¸ PENDIENTES para completar paridad con Docker

---

### ğŸŸ¡ 1. Migrar PostgreSQL a namespace `data` (CORTO PLAZO)  
**ClasificaciÃ³n:** Necesario para completar el patrÃ³n

**En Docker:** Postgres estaba en `db_net` (red dedicada)  
**En K8s actual:** Postgres estÃ¡ en `default` (temporal por migraciÃ³n de PVC)

**Estado:**  
- Funcional con workaround (NetworkPolicy especial)  
- â— Pero NO es la arquitectura ideal

**RazÃ³n:**  
Aunque funciona, no refleja fielmente el patrÃ³n Docker donde cada *tier* estÃ¡ completamente aislado. Actualmente existe una excepciÃ³n temporal.

---

### ğŸŸ¡ 2. Implementar namespace `edge` con `mobile-reverse-proxy` (CORTO PLAZO)  
**ClasificaciÃ³n:** Necesario si tienes trÃ¡fico mÃ³vil

**En Docker:** `mobile_nginx` estaba en `public_net` + `orchestration_net`  
**En K8s actual:** Namespace `edge` existe pero estÃ¡ vacÃ­o (sin deployment)

**Estado:**  
- âœ… Namespace creado  
- âœ… NetworkPolicies configuradas  
- âŒ Falta: Deployment de `mobile-reverse-proxy`

**RazÃ³n:**  
Si en Docker tenÃ­as `mobile-reverse-proxy`, debe existir en K8s para lograr paridad completa.


---

## ğŸ“ Referencias y Patrones

### PatrÃ³n de SegmentaciÃ³n en Docker (Original)

**Redes Docker**:
- `public_net` â†’ Reverse proxies pÃºblicos
- `frontend_net` â†’ Frontend SSR
- `orchestration_net` â†’ API Gateway
- `backend_net` â†’ Microservicios
- `db_net` â†’ Bases de datos

**Regla**: Cada container conectado **solo a las redes necesarias** para su funciÃ³n.

### TraducciÃ³n a Kubernetes

| Docker Concept | Kubernetes Equivalent |
|----------------|----------------------|
| Docker Network | Namespace |
| Network membership | Namespace + Labels |
| Firewall (implÃ­cito) | NetworkPolicy |
| docker-compose networks | namespaceSelector en NetworkPolicy |
| Service discovery | kube-dns (automÃ¡tico) |

### Arquitectura de Referencia

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          INTERNET (pÃºblico)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTPS
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Tier 0: Security â”‚  â† Ingress NGINX + TLS + WAF
       â”‚  (ingress-nginx)  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ HTTP
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Tier 1: Presentationâ”‚ â† Frontend SSR (NextJS)
       â”‚   (presentation)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚Tier 3: Orchestrationâ”‚ â† API Gateway (Express)
       â”‚  (orchestration)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  Tier 5: Backend   â”‚ â† Auth, Routes, Distance, Notifications
       â”‚    (backend)       â”‚    + RabbitMQ
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   Tier 7: Data     â”‚ â† PostgreSQL + PostGIS
       â”‚    (default*)      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âœ… ValidaciÃ³n Final

### Checklist de ImplementaciÃ³n

- [x] Namespaces creados y etiquetados
- [x] Pods corriendo en namespaces correctos
- [x] Services con ClusterIP en cada namespace
- [x] Ingresses configurados (frontend, mobile)
- [x] NetworkPolicies aplicadas (default deny + allows)
- [x] DNS resolution funcionando (kube-dns)
- [x] Conectividad entre tiers adyacentes verificada
- [x] SegmentaciÃ³n bloqueando trÃ¡fico no permitido
- [x] PostgreSQL accesible desde backend
- [x] RabbitMQ escalado y corriendo

### Comandos de ValidaciÃ³n RÃ¡pida

```powershell
# Ver todos los pods por tier
kubectl get pods -A | Select-String -Pattern "presentation|orchestration|backend|default.*postgres"

# Ver NetworkPolicies
kubectl get networkpolicies -A

# Ver servicios
kubectl get svc -A | Select-String -Pattern "frontend|api-gateway|auth|routes|distance|notification|postgres|rabbitmq"

# Probar conectividad permitida
kubectl exec -n presentation $(kubectl get pod -n presentation -o name | Select-Object -First 1) -- wget -O- --timeout=5 http://api-gateway-service.orchestration.svc.cluster.local

# Probar segmentaciÃ³n (debe fallar con timeout)
kubectl exec -n presentation $(kubectl get pod -n presentation -o name | Select-Object -First 1) -- timeout 5 wget -O- http://postgres.default.svc.cluster.local:5432
```

---

## ğŸ“ ConclusiÃ³n

El **Network Segmentation Pattern** estÃ¡ completamente implementado en RunPath usando Kubernetes nativo, replicando y mejorando la arquitectura de Docker:

âœ… **Aislamiento por capas** con namespaces  
âœ… **Control de trÃ¡fico** con NetworkPolicies  
âœ… **Principio de mÃ­nimo privilegio** (default deny + allows explÃ­citos)  
âœ… **Defensa en profundidad** (mÃºltiples capas de seguridad)  
âœ… **Escalabilidad** y **observabilidad** nativa de Kubernetes  

**Siguiente fase**: Migrar postgres a `data` namespace y agregar WAF/Rate Limiting en Ingress.
